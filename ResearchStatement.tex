%% start of file `template.tex'.
%% Copyright 2006-2015 Xavier Danaux (xdanaux@gmail.com).
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License version 1.3c,
% available at http://www.latex-project.org/lppl/.


\documentclass[11pt,a4paper]{moderncv}        % possible options include font size ('10pt', '11pt' and '12pt'), paper size ('a4paper', 'letterpaper', 'a5paper', 'legalpaper', 'executivepaper' and 'landscape') and font family ('sans' and 'roman')
\usepackage{ascii}

% moderncv themes
\moderncvstyle{casual}                             % style options are 'casual' (default), 'classic', 'banking', 'oldstyle' and 'fancy'
\moderncvcolor{blue}                               % color options 'black', 'blue' (default), 'burgundy', 'green', 'grey', 'orange', 'purple' and 'red'
\renewcommand{\familydefault}{\sfdefault}         % to set the default font; use '\sfdefault' for the default sans serif font, '\rmdefault' for the default roman one, or any tex font name
%\nopagenumbers{}                                  % uncomment to suppress automatic page numbering for CVs longer than one page

% character encoding
\usepackage[utf8]{inputenc}                 
% if you are not using xelatex ou lualatex, replace by the encoding you are using


% adjust the page margins
\usepackage[scale=0.75]{geometry}
%\setlength{\hintscolumnwidth}{3cm}                % if you want to change the width of the column with the dates
%\setlength{\makecvtitlenamewidth}{10cm}           % for the 'classic' style, if you want to force the width allocated to your name and avoid line breaks. be careful though, the length is normally calculated to avoid any overlap with your personal info; use this at your own typographical risks...
% \setlength{\parindent}{1em}
\usepackage{cite}

% personal data
\name{Niccol\`o}{Laurenti}
%\title{Resum√© title}                               % optional, remove / comment the line if not wanted
% \address{Via Leonida Rech, 80}{00156}{Roma (RM), Italia}% optional, remove / comment the line if not wanted; the "postcode city" and "country" arguments can be omitted or provided empty
\phone[mobile]{+39~338 2971956}
\email{niccolo.laurenti@mi.infn.it}
% \homepage{niclaurenti.github.io}                       % optional, remove / comment the line if not wanted

% bibliography adjustements (only useful if you make citations in your resume, or print a list of publications using BibTeX)
%   to show numerical labels in the bibliography (default is to show no labels)
\makeatletter\renewcommand*{\bibliographyitemlabel}{\@biblabel{\arabic{enumiv}}}\makeatother
%   to redefine the bibliography heading string ("Publications")
\renewcommand{\refname}{References}
\title{Research Statement}
% bibliography with mutiple entries
%\usepackage{multibib}
\bibliographystyle{unsrt}
%\newcites{book,misc}{{Books},{Others}}
%----------------------------------------------------------------------------------
%            content
%----------------------------------------------------------------------------------
\begin{document}

\makecvtitle

Starting with my Master thesis and then during my Ph.D., my research focused on phenomenological aspects of high energy physics (HEP)
and in particular on Quantum Chromodynamics (QCD).
I am involved in developing methods and computational tools to provide high precision theoretical predictions in HEP.
% This is of fundamental importance since with the start of the high-luminosity era at the LHC, experimental measurements will highly 
% increase their precision.
% Therefore, to effectively compare theory with experiments, theoretical calculations must attain a precision on the order of percent.

% In this context, a building block for the theory predictions are the parton distribution functions (PDFs).
% As these functions play a crucial role in computing every hadronic observable, their extraction from
% experimental data must be carried out with high precision.
% To achieve this goal, the methodological and computational techniques employed in their determination play a fundamental role. 
% Another essential aspect in PDFs extraction is the theoretical description of deep-inelastic-scattering (DIS) data (i.e.\ electron-proton
% scattering), since it imposes a very strong constraint on the PDF fits.

My research covered mainly the topics of PDF fits and theory predictions in deep-inelastic-scattering (DIS).
Indeed, on one hand I worked as a member of the NNPDF collaboration and as a developer of the NNPDF code to include
Quantum Electrodynamics (QED) effects in PDF fits.
On the other hand, I worked to develope a new scheme designed to increase the precision of DIS predictions.

I'm interested also in computational problems like machine learning but also in more theoretical ones related to phenomenology in 
HEP like resummation or fixed order calculations.

\section{Construction of an N$^3$LO DIS scheme}

During my Master Thesis I worked on the development of a so-called variable flavor number scheme (VFNS) 
for DIS predictions.
%
% This is needed to correctly consider the heavy quarks mass effects when computing theory predictions in DIS,
% whose understanding is crucial in PDF fits.
%
Our construction, unlike the ones that are available in the literature, takes into account the fact that the heavy quark PDF is generated
perturbatively and therefore must be accounted as $\mathcal{O}(\alpha_s)$.
%
We developed it up to next-to-next-to-leading order (NNLO) in perturbation theory, where all the needed ingredients are known.
Moreover, we extended it to N$^3$LO using approximate results for the unknown terms.
Even though I was also involved in the construction of the scheme, the bulk of the work I did was to
construct an approximation of the unknown terms of the N$^3$LO partonic cross section for DIS (the so-called coefficient functions) by
combining some known limits.

% This result is important since it allows a better understanding of DIS theoretical predictions on two levels.
% Fist of all our VFNS will allow a more precise treatment of the heavy quark PDF.
% Moreover, the inclusion of the N$^3$LO corrections, even if in an approximate form, will increase the perturbative convergence of the series.
% Both points will allow a better description of the DIS data
% and therefore they will increase the accuracy af the PDF fits.

As a result of this work two codes were written to produce the results.
Both of them are now public.
The first one, \href{https://github.com/niclaurenti/adani}{\texttt{Adani}}, is a \texttt{C++} code 
which computes both the approximation of the unknown terms and the exact value of the known ones
for the massive coefficient functions at N$^3$LO.
% The reason why the \texttt{C++} language was chosen is that in the N$^3$LO massive coefficient function there are some terms that are
% exactly known but only in numerical form since they are obtained through the integration of the lowest order coefficient functions and
% splitting functions.
% Therefore, a compiled code was more suitable to handle the heavy integrals that need to be computed.
%
The second one, \href{https://github.com/andreab1997/DIS_TP}{\texttt{DIS\_TP}}, is a \texttt{Python} code whose aim is to combine the different ingredients 
(like the output of the first code) to actually
compute the theory predictions with our scheme.
%
Regarding this work, a publication is in preparation.

\section{PDF fit}

During my Ph.D.\ my main project has been the inclusion of QED effects in PDF fits.
The goal of the work was to produce the QED fit of the NNPDF4.0 series that had to replace the previous QED fit from NNPDF, i.e.\ the NNPDF3.1QED PDF set.
In order to achieve it, different things had to be implemented in different codes.

The first step has been the implementation of QED corrections to the PDFs evolution equations, i.e.\ the DGLAP equations.
% They are a system of integro-differential equations, whose solution is highly non-trivial and therefore can be obtained only numerically.
They have been implemented in the public code \href{https://github.com/NNPDF/eko}{\texttt{EKO}}:
before I joined the NNPDF collaboration, \texttt{EKO} could handle only the pure QCD evolution, but I extended it to consider also QED corrections.
% This passes through various steps: find a PDFs basis that maximally decouple the system in presence of QED effects, write the DGLAP equations in this basis,
% compute the Mellin transform of the known QED corrections of the splitting functions
% (i.e.\ the $\mathcal{O}(\alpha)$, $\mathcal{O}(\alpha \alpha_s)$ and $\mathcal{O}(\alpha^2)$) and analytically continue them in the complex Mellin space,
% find a suitable numerical solution of the system in Mellin space and in the end compute the inverse Mellin transform.
%
The second step has been the extension of the \href{https://github.com/NNPDF/nnpdf}{NNPDF fitting code} to consider also
a photon PDF.
% Indeed, once we consider QED effects, we allow photons emissions from the quarks inside the proton and therefore we have to consider also a photon PDF.
This is obtained through the LuxQED method~\cite{Manohar:2016nzj, Manohar:2017eqh}: with this approach the photon PDF is linked through
a perturbative calculation to the quark and gluon PDFs.
% Since the structure functions themselves depend on the quarks and gluon PDFs, the LuxQED method links the photon PDF to the ``QCD'' PDFs.
In order to implement this method in the fitting framework, I interfaced the \texttt{FiatLux} code, a public code written in \texttt{C++} that implements the LuxQED formula, with the NNPDF code.
% In this way the fitting code is able to consider also the photon PDF: this one enters in the fit in photon initiated processes, in the DGLAP evolution and in the momentum sum rules.
A publication for this work is in preparation and preliminary results were presented in~Ref.\cite{LAURENTI2023}.

I was also involved in the approximate N$^3$LO PDFs determination by NNPDF.
% Indeed, DIS predictions and DGLAP evolution are completely known at NNLO, while at N$^3$LO there are still some missing informations
% (like the terms that I approximated during my Master thesis).
The aim of this project was to include the N$^3$LO corrections in DIS predictions and in PDFs evolution,
using the exact results when available and approximate ones for the missing terms.
I was mainly involved in the implementation of the approximation for the N$^3$LO massive DIS coefficient functions, that I developed
during my Master thesis, in the NNPDF code.
% In order to do it, I interfaced the code I discussed about in the past section with the NNPDF code.
% This involves some technical problems like the interface of a \texttt{C++} code with a \texttt{Python} one, and the distribution of
% a compiled package for all possible operative systems.
Also for this work a publication is in preparation.

\section{Side projects}

During my Ph.D.\ I also had the opportunity to work on different side projects.
%
One of these was the use of resummation results to approximate the missing higher orders for processes with colorless final states,
like Higgs production and Drell-Yan:
% One of the most common ways to estimate missing higher orders is with the use of the so-called scale variations:
% in the physical cross sections some unphysical scales appear, whose particular choice doesn't have to affect the all-orders result.
% However, when truncating the series, a dependence appears.
% It means that, varying these scales, it is possible to estimate the size of the unknown higher orders.
% This procedure has some caveats: the result of the scale variation has no probabilistic interpretation and the chioce of
% the central scale as well as the size of the variation are totally arbitrary.
% An alternative way of estimate (or approximate) the missing higher orders is using resummation: 
given that the resummed expressions
contains to all-orders the informations on the various kinematic limits that appear in the fixed order calculations, it is possible
to combine such limits to approximate the unknown higher order terms.
This has been done for Higgs production, yelding resonable predictions, while for Drell-Yan it is still a work in progress.
This work was presented in Ref.~\cite{laurenti2022approximating}.
In this context I had the chance to deepen my understanding of resummation, and in particular threshold and small-$p_T$ resummations.

\section{Other interests}

As a member of the NNPDF collaboration, I have great interest
in computational problems like machine learning and neural networks.
Even though I didn't write directly the neural network fitting code (as mentioned before, I extended it to consider also the photon PDF)
I have interest in this field, that is of great importance for the power of such tool and for spread that it had in the recent years
also in accademia.

Despite I spent the most of my Ph.D.\ on the PDF fits, I have also great interests in more theoretical topics of phenomenology in HEP,
like resummation or fixed order calculation, that are needed in order to reach the percent accuracy in theoretical predictions.

% \newpage

% Bibliography
\bibliography{biblio}


\end{document}


