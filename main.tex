%% start of file `template.tex'.
%% Copyright 2006-2015 Xavier Danaux (xdanaux@gmail.com).
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License version 1.3c,
% available at http://www.latex-project.org/lppl/.


\documentclass[11pt,a4paper]{moderncv}        % possible options include font size ('10pt', '11pt' and '12pt'), paper size ('a4paper', 'letterpaper', 'a5paper', 'legalpaper', 'executivepaper' and 'landscape') and font family ('sans' and 'roman')
\usepackage{ascii}

% moderncv themes
\moderncvstyle{casual}                             % style options are 'casual' (default), 'classic', 'banking', 'oldstyle' and 'fancy'
\moderncvcolor{blue}                               % color options 'black', 'blue' (default), 'burgundy', 'green', 'grey', 'orange', 'purple' and 'red'
\renewcommand{\familydefault}{\sfdefault}         % to set the default font; use '\sfdefault' for the default sans serif font, '\rmdefault' for the default roman one, or any tex font name
%\nopagenumbers{}                                  % uncomment to suppress automatic page numbering for CVs longer than one page

% character encoding
\usepackage[utf8]{inputenc}                 
% if you are not using xelatex ou lualatex, replace by the encoding you are using


% adjust the page margins
\usepackage[scale=0.75]{geometry}
%\setlength{\hintscolumnwidth}{3cm}                % if you want to change the width of the column with the dates
%\setlength{\makecvtitlenamewidth}{10cm}           % for the 'classic' style, if you want to force the width allocated to your name and avoid line breaks. be careful though, the length is normally calculated to avoid any overlap with your personal info; use this at your own typographical risks...
% \setlength{\parindent}{1em}
\usepackage{cite}

% personal data
\name{Niccol\`o}{Laurenti}
%\title{Resum√© title}                               % optional, remove / comment the line if not wanted
% \address{Via Leonida Rech, 80}{00156}{Roma (RM), Italia}% optional, remove / comment the line if not wanted; the "postcode city" and "country" arguments can be omitted or provided empty
\phone[mobile]{+39~338 2971956}
\email{niccolo.laurenti@mi.infn.it}
% \homepage{niclaurenti.github.io}                       % optional, remove / comment the line if not wanted

% bibliography adjustements (only useful if you make citations in your resume, or print a list of publications using BibTeX)
%   to show numerical labels in the bibliography (default is to show no labels)
\makeatletter\renewcommand*{\bibliographyitemlabel}{\@biblabel{\arabic{enumiv}}}\makeatother
%   to redefine the bibliography heading string ("Publications")
\renewcommand{\refname}{References}
\title{Research Statement}
% bibliography with mutiple entries
%\usepackage{multibib}
\bibliographystyle{unsrt}
%\newcites{book,misc}{{Books},{Others}}
%----------------------------------------------------------------------------------
%            content
%----------------------------------------------------------------------------------
\begin{document}

\makecvtitle

Starting with my master thesis and then during my Ph.D., my research focused on phenomenological aspects of high energy physics (HEP)
and in particular on the theory describing strong interactions, i.e.\ Quantum Chromodynamics (QCD).
I am involved in developing methods and computational tools to provide high precision theoretical predictions in HEP.
This is of fundamental importance since with the start of the high-luminosity era at the LHC, experimental measurements will highly 
increase their precision.
Therefore, to effectively compare theory with experiments, theoretical calculations must attain a precision on the order of percent.

In this context, a building block for the theory predictions are the parton distribution functions (PDFs).
As these functions play a crucial role in computing every hadronic observable, their extraction from
experimental data must be carried out with high precision.
To achieve this goal, the methodological and computational techniques employed in their determination play a fundamental role. 
Another essential aspect in PDFs extraction is the theoretical description of deep-inelastic-scattering (DIS) data (i.e.\ electron-proton
scattering), since it imposes a very strong constraint on the PDFs fit.

My research covered both these aspects.
On one hand I worked as a member of the NNPDF collaboration both on the theory side and as a developer of the NNPDF code to provide
computational tools capable to determine PDFs with a percent accuracy.
On the other hand, I worked to develope a new scheme designed to increase the theoretical comprehension of deep inelastic scattering
(DIS) predictions, thereby contributing to a more refined determination of PDFs.

\section{Construction of an N$^3$LO DIS scheme}

During my Master Thesis, under the supervision of Dr.\ Marco Bonvini and in collaboration with another Master student,
I worked on the development of a so-called variable flavor number scheme (VFNS) for DIS predictions.
This is needed to correctly consider the heavy quarks mass effects when computing theory predictions in DIS,
whose understanding is crucial in PDFs fit.
Different proposal exist in the literature and they are all equivalent at all order in perturbation theory but differ at any given
order in the way the different ingredients are combined.
What is fundamental to discuss is that all of these schemes consider the heavy quark PDF of $\mathcal{O}(\alpha_s^0)$.
Instead, our construction takes into account the fact that the heavy quark PDFs are generated
perturbatively and therefore have to be considered of $\mathcal{O}(\alpha_s)$ (this is true in the case in which intrinsic components
are neglected, while in the case in which they are considered, the scheme will
have to be modified disentangling the intrinsic part from the perturbatively generated one).

We developed it up to next-to-next-to-leading order (NNLO) in perturbation theory, where all the needed ingredients are known.
Moreover, we extended it at N$^3$LO using approximate results for the unknown terms.
Indeed, the ingredients needed for such a construction are completely known up to NNLO, while at N$^3$LO
there are still some missing informations.
Even though I was also involved in the construction of the scheme, the bulk of the work I did during my Master thesis was to
construct an approximation of the unknown terms of the N$^3$LO partonic cross section for DIS (the so-called coefficient functions) by
combining some known limits.
In this way it was possible to construct a VFNS at approximate N$^3$LO.

% \textcolor{red}{Ho approfondito bla bla bla}

This result is important since it allows a better understanding of DIS theoretical predictions on two levels.
Fist of all our VFNS will allow a more precise treatment of the heavy quark PDF.
Moreover, the inclusion of the N$^3$LO corrections, even if in an approximate form, will increase the perturbative convergence of the series.
Both points will allow a better description of the DIS data
and therefore they will increase the accuracy af the PDFs fits, that are fundamental for every theory prediction in hadronic collisions.

As a result of this work two codes where written to produce the results.
Both of them are now public.
The first one, \url{https://github.com/niclaurenti/adani}, is a \texttt{C++} code 
which computes the approximation of the heavy quark coefficient functions at N$^3$LO.
The reason why the \texttt{C++} language was chosen is that in the N$^3$LO massive coefficient function there are some terms that are
exactly known but only in numerical form since they are obtained through the integration of the lowest order coefficient functions and
splitting functions.
Therefore, a compiled code was more suitable to handle the heavy integrals that need to be computed.

The second is a \texttt{Python} code, \url{https://github.com/andreab1997/DIS_TP}, whose aim is to combine the different ingredients 
(like the output of the first code) to actually
compute the theory predictions with our scheme.

Regarding this work, a publication is in preparation.

\section{PDFs fit}

During my Ph.D.\ I continued with study of phenomenology in high energy phisics under the supervision of Prof.\ Stefano Forte.
In particular I focused on the topic of PDFs fits: PDFs are objects that link the hadronic cross sections,
that are what is actually observed experimentally, to partonic cross sections, that are what is computed theoretically.
Being non-perturbative objects, the PDFs cannot be computed in perturbation theory but they must be fitted from data.
Their extraction is a fundamental step in the computation of theory predictions in hadronic collisions, like the ones happening 
at the LHC, since the enter in the computation of every observable.
I delved into this subject when I joined the NNPDF collaboration, where I had the opportunity to study both the theoretical and the
computational aspects.

My main project has been the inclusion of Quantum Electrodynamics (QED) effects in PDFs fit.
The goal of the work was to produce the QED fit of the NNPDF4.0 series that had to replace the previous QED fit, i.e.\ the NNPDF3.1QED PDF set.
In order to achieve it, different things had to be implemented in different codes.

The first step is the implementation of QED corrections to the PDFs evolution equations, i.e.\ the DGLAP equations.
They are a system of integro-differential equations, whose solution is highly non-trivial and therefore can be obtained only numerically.
The solution of DGLAP equations, in presence of QED effects, has been implemented in the public code \texttt{EKO}
(\url{https://github.com/NNPDF/eko}), a code written in \texttt{Python} that computes the solution of the DGLAP equation with the
technique of the Mellin transform.
Before I joined the NNPDF collaboration, \texttt{EKO} could handle only the pure QCD case, but I extended it to consider also QED corrections.
This passes through various steps: find a PDFs basis that maximally decouple the system in presence of QED effects, write the DGLAP equations in this basis,
compute the Mellin transform of the known QED corrections of the splitting functions
(i.e.\ the $\mathcal{O}(\alpha)$, $\mathcal{O}(\alpha \alpha_s)$ and $\mathcal{O}(\alpha^2)$) and analytically continue them in the complex Mellin space,
find a suitable numerical solution of the system in Mellin space and in the end compute the inverse Mellin transform.

The second step has been the extension of the NNPDF fitting code (\url{https://github.com/NNPDF/nnpdf}) to consider also
a photon PDF.
Indeed, once we consider QED effects, that means that we allow photons emissions from the quarks inside the proton, we have to consider also
a photon PDF.
This is obtained through the LuxQED method~\cite{Manohar:2016nzj, Manohar:2017eqh}: with this approach the photon PDF is linked through
a perturbative calculation to the DIS structure functions.
Since the structure functions themselves depend on the quarks and gluon PDFs, the LuxQED method links the photon PDF to the ``QCD'' PDFs.
In this context, I interfaced the \texttt{FiatLux} code, a public code written in \texttt{C++} that implements the LuxQED formula, with the NNPDF code.
In this way the fitting code is able to include the photon contribution in photon initiated processes, DGLAP evolution and in momentum sum rules.

A publication for this work in in preparation and preliminary results were presented in~Ref.\cite{LAURENTI2023}.

I was also involved in the approximate N$^3$LO PDFs determination by NNPDF.
Indeed, almost all the ingredients needed for a PDF fit are exactly known up to NNLO (this is precisely true only for DIS predictions, as I mentioned in the past section, but not for all the proton-proton
collision processes whose data are included in the fit).
The aim of this project was to include the N$^3$LO corrections in DIS predictions and in PDFs evolution,
using the exact results when available and approximate ones for the missing terms.
I was mainly involved in the implementation of the approximate N$^3$LO massive DIS coefficient functions in the NNPDF code.
In order to do it, I interfaced the code I discussed about in the past section with the NNPDF code.
This involves some technical problems like the interface of a \texttt{C++} code with a \texttt{Python} one, and the distribution of
a compiled package for all possible operative systems. %\textcolor{red}{Parte un po' inutile}


\section{Side projects}

During my Ph.D.\ I also had the opportunity to work on different side projects.

One of these was the use of resummation results to approximate the missing higher orders for processes with colorless final states,
like Higgs production and Drell-Yan.
One of the most common ways to estimate missing higher orders is with the use of the so-called scale variations:
in the physical cross sections some unphysical scales appear, whose particular choice doesn't have to affect the all-orders result.
However, when truncating the series, a dependence appears.
It means that, varying these scales, it is possible to estimate the size of the unknown higher orders.
This procedure has some caveats: the result of the scale variation has no probabilistic interpretation and the chioce of
the central scale as well as the size of the variation are totally arbitrary.
An alternative way of estimate (or approximate) the missing higher orders is using resummation: given that the resummed expressions
contains to all-orders the informations on the variious kinematic limits that appear in the fixed order calculations, it is possible
to combine such limits to approximate the unknown higher order terms.
This has been done for Higgs production, yelding resonable predictions, while for Drell-Yan it is still a work in progress.
This work was presented in~ Ref.\cite{laurenti2022approximating}.
In this context I had the chance to deepen my understanding of resummation, and in particular threshold and small-$p_T$ resummations.

\section{Other interests}

As a member of the NNPDF collaboration, that uses neural networks to extract PDFs from experimental data, I have also a great interest
in computational problems like machine learning and neural networks.
Even though I didn't write directly the neural network fitting code (as mentioned before, I extended it to consider also the photon PDF)
I have interest in this field, that is of great importance for the power of such tool and for spread that it had in the recent years
also in accademia.

\newpage

% Bibliography
\bibliography{biblio}


\end{document}


